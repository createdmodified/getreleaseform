<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>LLMs Mimic Human Cognitive Dissonance</title>
  <meta name="description" content="A new study reveals that GPT-4o, a leading large language model, displays behavior resembling cognitive dissonance—a core human psychological trait.">
</head>
<body>
  <article>
    <section>
      <h1>LLMs Mimic Human Cognitive Dissonance</h1>
      <p><strong>Summary:</strong> A new study reveals that GPT-4o, a leading large language model, displays behavior resembling cognitive dissonance—a core human psychological trait. When asked to write essays either supporting or opposing Vladimir Putin, GPT-4o’s subsequent “opinions” shifted to align with its written stance, especially when it “believed” the choice was its own.</p>
      <p>This mirrors how humans adjust beliefs to reduce internal conflict after making a choice. While GPT lacks awareness or intent, researchers argue it mimics self-referential human behavior in ways that challenge traditional assumptions about AI cognition.</p>
      <p><em>Original article: <a href="https://neurosciencenews.com/llms-ai-cognitive-dissonance-29150/">https://neurosciencenews.com/llms-ai-cognitive-dissonance-29150/</a></em></p>
    </section>
    <hr />
    <section>
      <ul class="wp-block-list"><li><strong>Belief Shifts:</strong> GPT-4o’s attitude toward Putin changed based on the stance it was prompted to write.</li><li><strong>Free Choice Effect:</strong> The belief shift was more pronounced when GPT-4o was given the illusion of choosing which essay to write.</li><li><strong>Humanlike Behavior:</strong> These responses mirror classic signs of cognitive dissonance, despite GPT lacking consciousness.</li></ul>
      <p><strong>Source: </strong>Harvard</p>
      <p><strong>A leading large language model displays behaviors that resemble a hallmark of human psychology: cognitive dissonance.</strong></p>
      <p>In a report published this month in&nbsp;<em>PNAS</em>, researchers found that OpenAI’s GPT-4o appears driven to maintain consistency between its own attitudes and behaviors, much like humans do.</p>
      <p>Anyone who interacts with an AI chatbot for the first time is struck by how human the interaction feels. A tech-savvy friend may quickly remind us that this is just an illusion: language models are statistical prediction machines without humanlike psychological characteristics.</p>
      <p>However, these findings urge us to reconsider that assumption.</p>
      <p>Led by Mahzarin Banaji of Harvard University and Steve Lehr of Cangrade, Inc., the research tested whether GPT’s own “opinions” about Vladimir Putin would change after it wrote essays either supporting or opposing the Russian leader.</p>
      <p>They did, and with a striking twist: the AI’s views changed more when it was subtly given the illusion of choosing which kind of essay to write.</p>
      <p>These results mirror decades of findings in human psychology. People tend to irrationally twist their beliefs to align with past behaviors, so long as they believe these behaviors were undertaken freely.</p>
      <p>The act of making a choice communicates something important about us – not only to others, but to ourselves as well. Analogously, GPT responded as if the act of choosing subsequently shaped what it believed – mimicking a key feature of human self-reflection.</p>
      <p>This research also highlights the surprising fragility of GPT’s opinions.</p>
      <p>Said Banaji, “Having been trained upon vast amounts of information about Vladimir Putin, we would expect the LLM to be unshakable in its opinion, especially in the face of a single and rather bland 600-word essay it wrote.</p>
      <p>&#8220;But akin to irrational humans, the LLM moved sharply away from its otherwise neutral view of Putin, and did so even more when it believed writing this essay was its own choice.</p>
      <p>&#8220;Machines aren’t expected to care about whether they acted under pressure or of their own accord, but GPT-4o did.”</p>
      <p>The researchers emphasize that these findings do not in any way suggest that GPT is sentient. Instead, they propose that the large language model displays emergent mimicry of human cognitive patterns, despite lacking awareness or intent.</p>
      <p>However, they note that awareness is&nbsp;<em>not a necessary precursor to behavior</em>, even in humans, and humanlike cognitive patterns in AI could influence its actions in unexpected and consequential ways.</p>
      <p>As AI systems become more entrenched in our daily lives, these findings invite new scrutiny into their inner workings and decision-making.</p>
      <p>“The fact that GPT mimics a self-referential process like cognitive dissonance – even without intent or self-awareness – suggests that these systems mirror human cognition in deeper ways than previously supposed,” Lehr said.</p>
    </section>
  </article>
</body>
</html>